{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H-CyrHRXmRl"
      },
      "source": [
        "Implementing a Marathi Language Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "88j4MY86XmRp"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "from nltk import ngrams, word_tokenize, sent_tokenize\n",
        "\n",
        "import scipy\n",
        "from scipy.sparse import dok_matrix, csr_matrix\n",
        "from scipy.spatial.distance import euclidean, cosine\n",
        "import numpy as np\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE-F7XU8XmRr",
        "outputId": "285afa4e-5e5b-4ed5-b28e-2f346fa18d1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Om\n",
            "[nltk_data]     Ambaye\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGqxX9R9XmRs"
      },
      "source": [
        "We will use the preprocessed corpus that we have already saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qZvQ7ZFAXmRu"
      },
      "outputs": [],
      "source": [
        "input_file = 'Corpus.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uIllat3KXmRu"
      },
      "outputs": [],
      "source": [
        "# function that generates all the unigrams from the file given\n",
        "x = Counter()\n",
        "with open(input_file, 'r',encoding=\"utf-8\") as f:\n",
        "    for line in f.readlines():\n",
        "        for sent in sent_tokenize(line):\n",
        "            x.update([i[0] for i in ngrams(word_tokenize(sent), 1)])\n",
        "vocab_counts=x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K9VE0pP0XmRw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary is 831270\n"
          ]
        }
      ],
      "source": [
        "# the size of vocabulary will be the number of entries in vocab_counts\n",
        "print(\"Size of vocabulary is\",len(vocab_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CHW-M-b_XmRx"
      },
      "outputs": [],
      "source": [
        "#stop words for marathi language\n",
        "a=\"अधिक अनेक अशी असलयाचे असलेल्या असा असून असे आज आणि आता आपल्या आला आली आले आहे आहेत एक एका कमी करणयात करून का काम काय काही किवा की केला केली केले कोटी गेल्या घेऊन जात झाला झाली झाले झालेल्या टा डॉ तर तरी तसेच ता ती तीन ते तो त्या त्याचा त्याची त्याच्या त्याना त्यानी त्यामुळे त्री दिली दोन न नाही निर्ण्य पण पम परयतन पाटील म मात्र माहिती मी मुबी म्हणजे म्हणाले म्हणून या याचा याची याच्या यांच्या याना यांचे यांची यांना यांनी यांच्या यानी येणार येत येथील येथे लाख व व्यकत सर्व सागित्ले सुरू हजार हा ही हे होणार होत होता होती होते\"\n",
        "a=a.split(\" \")\n",
        "stp = set(a)\n",
        "# creating a new vocab where only the words which are not stopwords and occur more than 100 times are considered\n",
        "vocab = {x for x, count in vocab_counts.items() if count >= 100 and x not in stp}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YnlTNouAXmRx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the reduced vocabulary is 12035\n"
          ]
        }
      ],
      "source": [
        "# size of the newly generated vocabulary\n",
        "print(\"Size of the reduced vocabulary is\",len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7cuul7TXmRy"
      },
      "source": [
        "Creating a dictionary which will act as an index, where the keys will be numbers and words where the values are the corresponding word and number respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v3ZX5mOTXmRy"
      },
      "outputs": [],
      "source": [
        "# number to word\n",
        "vocab_list = list(vocab)\n",
        "\n",
        "# corresponding word to number\n",
        "vocab_pos = {vocab_list[i] : i for i in range(len(vocab_list))}\n",
        "\n",
        "# compiling both into a dictionary\n",
        "vocab_idx = vocab_pos.copy()\n",
        "vocab_idx.update({i : w for i,w in enumerate(vocab_list)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73T6JwR7XmRz"
      },
      "source": [
        "## Building Co-occurence matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyJBPVdoXmRz"
      },
      "source": [
        "The function below returns a co-occurence matrix, which is in the format of a compressed sparse row matrix (scipy csr matrix).\n",
        "\n",
        "It will first create a defaultdict of counters, which will keep track of co-occurences where the ith index of the defaultdict corresponds to a counter which acts as a sparse vector of the word at the ith index.\n",
        "\n",
        "Then it creates an empty dok matrix and populates it the entries in the above defaultdict.\n",
        "\n",
        "Finally, it converts the dok matrix to a csr matrix and returns it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z71jpdLxXmRz"
      },
      "outputs": [],
      "source": [
        "def co_occurences(file):\n",
        "    ramp = [0,4,3,2,1]\n",
        "    # to keep track of co-occurences\n",
        "    occurences = defaultdict(lambda : Counter())\n",
        "\n",
        "    with open(file, 'r',encoding=\"utf-8\") as corpus:\n",
        "\n",
        "        for line in (corpus.readlines()):\n",
        "        # list of all n-grams in the line, where n = size of window + 1. Added padding so that all the possible co-occurences are listed\n",
        "            all_grams = ngrams(word_tokenize(line), 5, pad_right = True, pad_left = True)\n",
        "\n",
        "            for grams in all_grams:\n",
        "                # will proceed only if gram[0] is in the vocab.\n",
        "                if grams[0] in vocab :\n",
        "                    for idx, gram in enumerate(grams):\n",
        "                        if gram in vocab:\n",
        "                            # increase the co-occurence according to the distance between the words\n",
        "                            occurences[vocab_idx[grams[0]]][vocab_idx[gram]] += ramp[idx]\n",
        "\n",
        "                # Doing the same as above with the gram reversed\n",
        "                grams_rev = grams[::-1]\n",
        "\n",
        "                if grams_rev[0] in vocab:\n",
        "                    for idx, gram in enumerate(grams_rev):\n",
        "                        if gram in vocab:\n",
        "                            occurences[vocab_idx[grams_rev[0]]][vocab_idx[gram]] += ramp[idx]\n",
        "\n",
        "    # creating an empty sparse matrix\n",
        "    mat = dok_matrix((len(vocab), len(vocab)), dtype=np.int64)\n",
        "\n",
        "    # populating the matrix\n",
        "    for i in (range(len(vocab))):\n",
        "        for j in occurences[i].keys():\n",
        "            mat[i,j] = occurences[i][j]\n",
        "\n",
        "    # converting to csr matrix\n",
        "    return mat.tocsr()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rZ38UXCmXmR0"
      },
      "outputs": [],
      "source": [
        "co_occurence_matrix = co_occurences(input_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE-0ubqcMj5J"
      },
      "source": [
        "## Building the Word Embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Rw2ESoWUXmR1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from scipy.sparse import dok_matrix, csr_matrix\n",
        "import math\n",
        "\n",
        "def word_embedding(matrix):\n",
        "    total = matrix.sum()\n",
        "    row_sum = matrix.sum(axis=1).A.flatten()  # Use .A to convert to dense array\n",
        "    col_sum = matrix.sum(axis=0).A.flatten()  # Use .A to convert to dense array\n",
        "\n",
        "    vocab_size = matrix.shape[0]\n",
        "\n",
        "    mat = dok_matrix((vocab_size, vocab_size), dtype=np.float64)\n",
        "\n",
        "    indices = matrix.nonzero()\n",
        "    for a, b in (zip(*indices)):\n",
        "        a_sum = row_sum[a]\n",
        "        b_sum = col_sum[b]\n",
        "        num = total * matrix[a, b] - a_sum * b_sum\n",
        "        l1 = np.multiply(a_sum, (total - a_sum), dtype=np.float64)\n",
        "        l2 = np.multiply(b_sum, (total - b_sum), dtype=np.float64)\n",
        "        den = np.multiply(l1, l2, dtype=np.float64)\n",
        "        val = num / math.sqrt(float(den))\n",
        "        mat[a, b] = math.sqrt(val) if val > 0 else 0\n",
        "\n",
        "    return mat.tocsr()\n",
        "\n",
        "\n",
        "embmat = word_embedding(co_occurence_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylNguJcTMj5J"
      },
      "source": [
        "## Similarity measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z6lmTt0sXmR1"
      },
      "outputs": [],
      "source": [
        "def word_dist(x, y):\n",
        "    a1 = embmat[vocab_idx[x],:].toarray().flatten()\n",
        "    a2 = embmat[vocab_idx[y],:].toarray().flatten()\n",
        "    return cosine(a1, a2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "d01NVptaXmR1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.797710450044582"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#an example\n",
        "word_dist('आई', 'बाबा')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nenn_0IXmR2"
      },
      "source": [
        "This function will find the k closest words to the given word.\n",
        "\n",
        "The norms of all the vectors are pre calculated\n",
        "\n",
        "It calculates the vector of the given word and then loops through all the words in the vocab, create their vectors and find the distance and add that to a dist list along with the word.\n",
        "\n",
        "Then we sort the list with the distance and return the top k elements remopving the first one (as that will be the word itself)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "4UDcnxv7XmR2"
      },
      "outputs": [],
      "source": [
        "# precomputing the norms of all the vectors\n",
        "norms = [(lambda x: np.linalg.norm(co_occurence_matrix[x,:].toarray().flatten()))(x) for x in range(len(vocab))]\n",
        "\n",
        "def closest_words(x, k=10):\n",
        "    a1 = embmat[vocab_idx[x],:].toarray().flatten()\n",
        "    a1 = a1/norms[vocab_idx[x]]  # vector of the target word\n",
        "\n",
        "    dists = [(i, cosine(a1, embmat[vocab_idx[i],:].toarray().flatten())) for i in (vocab)]\n",
        "\n",
        "    dists.sort(key= lambda x:x[1])\n",
        "\n",
        "    return dists[0:k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "p0PDCSrBXmR6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('श्री', 0.6607320716186517),\n",
              " ('आमटे', 0.6640443953675746),\n",
              " ('महाराज', 0.6848877552895938),\n",
              " ('संत', 0.6877123492122967),\n",
              " ('गाडगे', 0.7170886870711456),\n",
              " ('ऊर्फ', 0.7239856673416889),\n",
              " ('नारायण', 0.7283487325929674),\n",
              " ('गुरू', 0.7306084832046923),\n",
              " ('मुरलीधर', 0.7438177965225354),\n",
              " ('प्रकाश', 0.7452581440965179)]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#one example\n",
        "closest_words('बाबा')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MygkiXZmXmR7"
      },
      "source": [
        "## Listing similar words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTUUhwG-XmR7"
      },
      "source": [
        "The nouns picked :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "yh53ajI9XmR7"
      },
      "outputs": [],
      "source": [
        "nouns = ['कामगार', 'सचिन', 'आंबा', 'गुरुवार','काश्मीर']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia_KXCpTXmR7"
      },
      "source": [
        "Computing the similar words and putting them in a list of list of similar words, where the first word is the target word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "VOgDJDeHXmR7"
      },
      "outputs": [],
      "source": [
        "similar = []\n",
        "for w in nouns:\n",
        "    similar.append([w]+[x for x,_ in closest_words(w, k=5)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "PDtKt-AhXmR7"
      },
      "outputs": [],
      "source": [
        "def print_similar_words(i):\n",
        "    print(\"For the word\", similar[i][0])\n",
        "    print(\"The top 5 similar words are\", str(similar[i][1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "NnZkHc68Mj5K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For the word कामगार\n",
            "The top 5 similar words are ['शेतमजूर', 'वीटभट्टीमजूर', 'नोकर', 'शेतकरी', 'सरकारी']\n"
          ]
        }
      ],
      "source": [
        "print_similar_words(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ocTLguGnMj5K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For the word सचिन\n",
            "The top 5 similar words are ['तेंडुलकर', 'कुलकर्णी', 'राहुल', 'प्रिया', 'रमेश']\n"
          ]
        }
      ],
      "source": [
        "print_similar_words(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "mv4uN014Mj5K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For the word आंबा\n",
            "The top 5 similar words are ['काजू', 'नारळ', 'भात', 'केळी', 'ऊस']\n"
          ]
        }
      ],
      "source": [
        "print_similar_words(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "g_7X7y42Mj5L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For the word गुरुवार\n",
            "The top 5 similar words are ['सोमवार', 'मंगळवार', 'शुक्रवार', 'रविवार', 'बुधवार']\n"
          ]
        }
      ],
      "source": [
        "print_similar_words(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "t7Q_7zRoMj5L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For the word काश्मीर\n",
            "The top 5 similar words are ['जम्मू', 'लडाख', 'काश्मीरमधील', 'पंजाब', 'केंद्रशासित']\n"
          ]
        }
      ],
      "source": [
        "print_similar_words(4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
